---
title: "RNAseq data summary"
author: "Cait McDonald"
date: "Last Updated: `r Sys.Date()`"
output:
  html_document:
    df_print: paged
    toc: true
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE)
start_time <- Sys.time()
```


```{r, message=FALSE, eval=F}
library(tidyverse)
library(here)
# dir.create("outputs/001", recursive = TRUE, showWarnings = FALSE) #no longer using this structure since I'm trying to use snakemake
# dir.create("intermediates/001", recursive = TRUE, showWarnings = FALSE)
# dir.create("scripts", recursive = TRUE, showWarnings = FALSE)
```

## Overview
We have 39 RNAseq libraries from the following samples (start a summit session and use opt+cmd+return to send to terminal):
```{bash, eval=F} 
ls /scratch/summit/camcd@colostate.edu/felv_rnaseq/data/samples/
```

Samples are named such as: `4438_S1_L002_R1_001.fastq.gz` in the `samples` directory. L002 and 001 are common to all files, and I assume indicate sequencing run info. I could shorten all filenames in `data` like so:

```{bash, eval=F}
for fname in *.fastq.gz ; do mv "$fname" "$(echo "$fname" | sed -r 's/L002_//')" ; done
for fname in *.fastq.gz ; do mv "$fname" "$(echo "$fname" | sed -r 's/_001//')" ; done
```

However, I think it's better to not mess with them, especially since I can use input functions and wildcards.

## What I want to do
I'd like to start using Snakemake, as recommended by Eric Anderson. It seems like a steep-ish learning curve???

Regardless, I want to:

1. run fastqc
1. run multiqc
1. trim
1. re-run fastqc on trimmed libraries
1. re-run multiqc on trimmed libraries

I have previously generated a shell script that does something to this effect:

```{bash, eval=F}
fastqc *.fastq.gz -o ./fastqc_raw -t 24 >& fastqc_raw.log #fastqc
multiqc ./fastqc_raw -o ./multiqc_raw #multiqc
parallel trim_galore --phred33 --length 36 -q 5 --stringency 1 -e 0.1 -o ./trim_galore_out/ {=s/_R1/_R2/=} ::: *_R1.fastq.gz
fastqc *.fastq.gz -o ./fastqc_trimmed -t 24 >& fastqc_trimmed.log #fastqc
multiqc ./fastqc_trimmed -o ./multiqc_trimmed #multiqc
```

I'm trying to turn that into a Snakemake workflow.

## Snakemake
One annoying thing is that Snakefile scripts are much nicer to write in Atom than Rmd. So not totally sure how I'll integrate everything? Also not sure about how to integrate Snakemake with SLURM.

After going through the Snakemake [tutorial](https://snakemake.readthedocs.io/en/stable/tutorial/tutorial.html), I've set up my directories and environments as suggested, and created a [Snakefile](workflow/Snakefile). One super useful utility is `tree`, which allows you to visualize directory structure. (I currently have this installed in the snakemake env.)
I went through quite a bit of troubleshooting and was able to get the first `fastqc_raw` rule to work with:

1. Full file names (pointless, but a good exercise)
1. A basic `{sample}` wildcard
1. A config file
1. A config file + input function
1. Importing metadata via pandas + input function

Via:
```{bash, eval=F}
conda activate snakemake
snakemake -np results/fastqc/raw/4438_S1_L002_R1_001_fastqc.html
```

__NOTE 1:__ If I try to run with a conda environment that includes both fastqc and multiqc, I get a PIP error:

          Building DAG of jobs...
          Creating conda environment workflow/envs/qctrim.yaml...
          Downloading and installing remote packages.
          CreateCondaEnvironmentException:
          Could not create conda environment from             /gpfs/summit/scratch/camcd@colostate.edu/felv_rnaseq/workflow/envs/qctrim.yaml:
          Preparing transaction: ...working... done
          Verifying transaction: ...working... done
          Executing transaction: ...working... done
          Installing pip dependencies: ...working... failed
          Pip subprocess error:
          DEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support
          ERROR: Could not find a version that satisfies the requirement multiqc==1.0.dev0 (from -r /gpfs/summit/scratch/camcd@colostate.edu/felv_rnaseq/.snakemake/conda/condaenv.1cma2cye.requirements.txt (line 1)) (from versions: 0.1, 0.2.0, 0.3.0, 0.3.1, 0.3.2, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 1.10, 1.10.1)
          ERROR: No matching distribution found for multiqc==1.0.dev0 (from -r /gpfs/summit/scratch/camcd@colostate.edu/felv_rnaseq/.snakemake/conda/condaenv.1cma2cye.requirements.txt (line 1))

This is because multiqc is downloaded via PIP and not conda and there are known errors when doing [both.](https://www.anaconda.com/blog/using-pip-in-a-conda-environment) Adding granularity and creating two separate environments for fastqc and multiqc resolves it.

__NOTE 2:__ 
I initially tried to use wildcards with truncated filenames (e.g. sample_id=4438_R1 instead of 4438_S1_L002_R1_001). While this works in the input function, I can't then propogate the shortened name because fastqc does not allow you to specify output filenames, so you can't propagate the shortened name. So, I have to use the full filename as sample_id.

My second rule (`multiqc_raw`) *looks like* it's working. I don't know for sure because I already generated all the files directly using a batch script (see below).

## SLURM scheduling

While I'm debugging my snakemake workflow, I just went ahead and submitted fastqc and multiqc via a bash script. I thought about doing it as a job array, but since multiqc isn't multi-threaded, I thought it made more sense to specify threads within each command, and then just request an adequate amount of memory and time instead. Because fastqc assigns 250MB per cpu, I requested 5GB (250 x 19 = 4750).

I submitted the [fastqc.sh](workflow/scripts/fastqc.sh) script via:

```{bash, eval=F}
sbatch workflow/scripts/fastqc.sh
```

With the `--test-only` flag first to make sure I wasn't requesting more memory than existed. This took ~2.5 hours to run. __Could this be better optimized?__
